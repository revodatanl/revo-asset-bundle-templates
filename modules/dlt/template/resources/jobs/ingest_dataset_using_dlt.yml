defaults: &default_task_config
  run_if: ALL_SUCCESS
  libraries:
    - whl: ../../dist/*.whl
  timeout_seconds: 0

resources:
  jobs:
    ingest_dataset_using_dlt:
      name: ingest_dataset_using_dlt

      tasks:
        - task_key: dlt_task
          pipeline_task:
            pipeline_id: ${resources.pipelines.dlt_ingest_dataset.id}

      max_concurrent_runs: 1

      schedule:
        # Run every day at 8:37 AM
        quartz_cron_expression: 44 37 8 * * ?
        timezone_id: Europe/Amsterdam

      job_clusters:
        - job_cluster_key: job_cluster
          new_cluster:
            spark_version: ${var.spark_version}}}
            node_type_id: Standard_D3_v2
            num_workers: 1
            spark_conf:
              spark.databricks.delta.optimizeWrite.enabled: true

  pipelines:
    dlt_ingest_dataset:
      name: dlt_ingest_dataset
      target: dlt
      catalog: main # this option enables Unity Catalog
      libraries:
        - notebook:
            path: ../notebooks/dlt_ingest_dataset.py

      clusters:
        - label: default
          num_workers: 1
          node_type_id: Standard_D4s_v3

      configuration:
        bundle.sourcePath: /Workspace${workspace.file_path}
        table: dlt_data
        autoloader_path: file:/Workspace${workspace.file_path}/fixtures
        bundle.env: ${bundle.environment}
        pipelines.allowCustomSchemaForSql: true
        pipelines.allowCustomSchemaForPython: true

      continuous: false
      photon: false
      development: true

      channel: PREVIEW
