# Development

## Project Structure

The project is structured as follows:

```text
{{.project_name}}/{{if (eq .cicd_provider "azure")}}
├── .azure/                   # Azure DevOps pipelines{{end}}{{if (eq .cicd_provider "github")}}
├── .github/                        # GitHub Actions workflows{{end}}
├── .vscode/                        # VSCode settings
├── bundle/                         # Bundle targets/variables configuration
├── docs/                           # Documentation
├── resources/                      # Databricks resources (jobs, pipelines)
├── scratch/                        # Personal, exploratory notebooks (gitignored)
├── src/                            # Source code
│   └── notebooks/                  # Databricks notebooks
├── tests/                          # Tests
├── .gitignore                      # Git ignore patterns
├── .pre-commit-config.yaml         # Pre-commit hooks configuration
├── .python-version                 # Python version specification
├── CHANGELOG.md                    # Project changelog
├── databricks.yml                  # Bundle configuration
├── Makefile                        # Development automation
├── pyproject.toml                  # Python project configuration
├── README.md                       # Project documentation
└── release.config.mjs              # Semantic release configuration
```

## Project Configuration

Projects include three key configuration files that work together to define the project structure and behavior:

### databricks.yml

The Databricks Asset Bundle configuration:

- **Bundle name**: Uses the project name from template parameters
- **Artifact configuration**: Defines Python wheel packaging using `uv build` with dynamic versioning
- **Resources**: Specifies Databricks resources like jobs and pipelines

### pyproject.toml

The Python project configuration file that defines:

- **Project metadata**: Name, version, description, author information, and Python version requirements
- **Dependencies**: Core dependencies are minimal by default, with comprehensive development dependencies including:
  - `databricks-connect` for local Databricks development
  - `databricks-sdk` and `databricks-dlt` for Databricks integration
  - Python tooling: `ruff`, `mypy`, `pytest`, `pre-commit`
- **Tool configuration**:
  - **Ruff**: Linting and formatting with RevoData's coding standards
  - **MyPy**: Type checking with strict mode enabled
  - **Pytest**: Test configuration with coverage reporting
  - **Pydoclint**: Documentation linting with NumPy-style docstrings

## Code Quality

We ~~encourage~~ enforce code quality with pre-commit hooks and the CI pipeline.

### Pre-commit Hooks

Ensure that the [`pre-commit`](https://pre-commit.com) hook defined in `.pre-commit-config.yaml` passes successfully:

- [`conventional-pre-commit`](https://github.com/compilerla/conventional-pre-commit) to enforce conventional commit standards.
- [`ruff`](https://docs.astral.sh/ruff/) for linting and formatting.
- [`mypy`](https://mypy.readthedocs.io/en/stable/) for static type checking (in `strict` mode)
- [`pydoclint`](https://github.com/shmsi/pydoclint) to lint docstrings.
- [`gitleaks`](https://github.com/gitleaks/gitleaks) to prevent sensitive information from being committed.

{{if (eq .cicd_provider "github")}}### Dependabot

We use GitHub's Dependabot to automatically check for updates to GitHub Actions and Python dependencies. It performs daily checks and groups dependencies as follows:

- **GitHub Actions**: All dependencies within GitHub Actions are monitored and updated weekly.
- **Python Dependencies**:
  - **Development Dependencies**: Development-specific dependencies are updated weekly.
  - **Production Dependencies**: Production-specific dependencies are also checked weekly.

This ensures dependencies are regularly maintained while adhering to the fixed environment constraints.{{end}}

## Local Development with Databricks Connect

Run local code on Databricks compute. Four connection methods:

| Method | Use Case | Configuration |
|--------|----------|---------------|
| **VS Code Extension** | Visual cluster selection | [Install extension](https://marketplace.visualstudio.com/items?itemName=databricks.databricks) |
| **Serverless** | Development, testing | `DatabricksSession.builder.serverless(True).getOrCreate()` |
| **Profile-based** | Multiple workspaces | `Config(profile="<name>", cluster_id="<id>")` |
| **Environment** | CI/CD pipelines | Set `DATABRICKS_CONFIG_PROFILE` and `DATABRICKS_HOST` in `.env` |

### Testing on (serverless) Databricks Connect

Tests are configured to automatically run on (serverless) Databricks Connect. This is configured in the `tests/conftest.py` file, which is automatically discovered by `pytest`. A spark session for testing is created using the `DatabricksSession` builder with serverless mode enabled.

```python
from databricks.connect import DatabricksSession
spark = DatabricksSession.builder.serverless(True).getOrCreate()
```

## Troubleshooting

Common issues and solutions:

1. **Authentication**: Ensure your `.databrickscfg` is properly configured
2. **Environment conflicts**: Check that your `.env` file doesn't conflict with system environment variables
3. **Cluster access**: Verify you have permission to access the specified cluster or serverless compute
4. **Version compatibility**: Ensure your local Databricks Connect version is compatible with your workspace
5. **Conflict with PySpark**: [Databricks Connect conflicts with PySpark](https://docs.databricks.com/aws/en/dev-tools/databricks-connect/python/troubleshooting#conflicting-pyspark-installations). Having both installed will cause errors when initializing the Spark context in Python.

...and of course:

![it-crowd](images/it-crowd.png)
